{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Mammographic+Mass)\n",
    "\n",
    "This data contains 961 instances of masses detected in mammograms, and contains the following attributes:\n",
    "\n",
    "\n",
    "   1. BI-RADS assessment: 1 to 5 (ordinal)  \n",
    "   2. Age: patient's age in years (integer)\n",
    "   3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "   4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "   5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "   6. Severity: benign=0 or malignant=1 (binominal)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload this:\n",
    "mammographic_masses.data.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shapee</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>66</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BI_RADS age shapee margin density  severity\n",
       "0        5  67      3      5       3         1\n",
       "1        4  43      1      1                 1\n",
       "2        5  58      4      5       3         1\n",
       "3        4  28      1      1       3         0\n",
       "4        5  74      1      5                 1\n",
       "5        4  65      1              3         0\n",
       "6        4  70                     3         0\n",
       "7        5  42      1              3         0\n",
       "8        5  57      1      5       3         1\n",
       "9        5  60             5       1         1\n",
       "10       5  76      1      4       3         1\n",
       "11       3  42      2      1       3         1\n",
       "12       4  64      1              3         0\n",
       "13       4  36      3      1       2         0\n",
       "14       4  60      2      1       2         0\n",
       "15       4  54      1      1       3         0\n",
       "16       3  52      3      4       3         0\n",
       "17       4  59      2      1       3         1\n",
       "18       4  54      1      1       3         1\n",
       "19       4  40      1                        0\n",
       "20          66                     1         1\n",
       "21       5  56      4      3       1         1\n",
       "22       4  43      1                        0\n",
       "23       5  42      4      4       3         1\n",
       "24       4  59      2      4       3         1\n",
       "25       5  75      4      5       3         1\n",
       "26       2  66      1      1                 0\n",
       "27       5  63      3              3         0\n",
       "28       5  45      4      5       3         1\n",
       "29       5  55      4      4       3         0\n",
       "30       4  46      1      5       2         0\n",
       "31       5  54      4      4       3         1\n",
       "32       5  57      4      4       3         1\n",
       "33       4  39      1      1       2         0\n",
       "34       4  81      1      1       3         0\n",
       "35       4  77      3                        0\n",
       "36       4  60      2      1       3         0\n",
       "37       5  67      3      4       2         1\n",
       "38       4  48      4      5                 1\n",
       "39       4  55      3      4       2         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DearDataset = pd.read_csv('mammographic_masses(with_Missing).data.txt')\n",
    "DearDataset.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you use the optional parmaters in read_csv to convert missing data (indicated by a ?) into NaN, and to add the appropriate column names (BI_RADS, age, shape, margin, density, and severity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DearDataset.severity.unique()    # check all the unique values so you can get rid of them if they are outliers or noise(NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DearDataset.drop(DearDataset[DearDataset.BI_RADS == ' '].index, inplace=True)\n",
    "DearDataset.drop(DearDataset[DearDataset.BI_RADS == 55].index, inplace=True)  # check your data for irrelevant values also!\n",
    "DearDataset.drop(DearDataset[DearDataset.BI_RADS == 6].index, inplace=True)\n",
    "DearDataset.drop(DearDataset[DearDataset.margin == ' '].index, inplace=True)\n",
    "DearDataset.drop(DearDataset[DearDataset.age == ' '].index, inplace=True)\n",
    "DearDataset.drop(DearDataset[DearDataset.density == ' '].index, inplace=True)\n",
    "DearDataset.drop(DearDataset[DearDataset.shapee == ' '].index, inplace=True)# I named it shapee because \"shape\" is a func in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shapee</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>829 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    BI_RADS age shapee margin density  severity\n",
       "0         5  67      3      5       3         1\n",
       "2         5  58      4      5       3         1\n",
       "3         4  28      1      1       3         0\n",
       "8         5  57      1      5       3         1\n",
       "10        5  76      1      4       3         1\n",
       "11        3  42      2      1       3         1\n",
       "13        4  36      3      1       2         0\n",
       "14        4  60      2      1       2         0\n",
       "15        4  54      1      1       3         0\n",
       "16        3  52      3      4       3         0\n",
       "17        4  59      2      1       3         1\n",
       "18        4  54      1      1       3         1\n",
       "21        5  56      4      3       1         1\n",
       "23        5  42      4      4       3         1\n",
       "24        4  59      2      4       3         1\n",
       "25        5  75      4      5       3         1\n",
       "28        5  45      4      5       3         1\n",
       "29        5  55      4      4       3         0\n",
       "30        4  46      1      5       2         0\n",
       "31        5  54      4      4       3         1\n",
       "32        5  57      4      4       3         1\n",
       "33        4  39      1      1       2         0\n",
       "34        4  81      1      1       3         0\n",
       "36        4  60      2      1       3         0\n",
       "37        5  67      3      4       2         1\n",
       "39        4  55      3      4       2         0\n",
       "41        4  78      1      1       1         0\n",
       "42        4  50      1      1       3         0\n",
       "44        5  62      3      5       2         1\n",
       "46        5  64      4      5       3         1\n",
       "..      ...  ..    ...    ...     ...       ...\n",
       "930       4  57      4      3       3         1\n",
       "931       4  55      2      1       2         0\n",
       "932       4  62      2      1       3         0\n",
       "933       4  54      1      1       3         0\n",
       "934       4  71      1      1       3         1\n",
       "935       4  65      3      3       3         0\n",
       "936       4  68      4      4       3         0\n",
       "937       4  64      1      1       3         0\n",
       "938       4  54      2      4       3         0\n",
       "939       4  48      4      4       3         1\n",
       "940       4  58      4      3       3         0\n",
       "941       5  58      3      4       3         1\n",
       "942       4  70      1      1       1         0\n",
       "943       5  70      1      4       3         1\n",
       "944       4  59      2      1       3         0\n",
       "945       4  57      2      4       3         0\n",
       "946       4  53      4      5       3         0\n",
       "947       4  54      4      4       3         1\n",
       "948       4  53      2      1       3         0\n",
       "949       0  71      4      4       3         1\n",
       "950       5  67      4      5       3         1\n",
       "951       4  68      4      4       3         1\n",
       "952       4  56      2      4       3         0\n",
       "953       4  35      2      1       3         0\n",
       "954       4  52      4      4       3         1\n",
       "955       4  47      2      1       3         0\n",
       "956       4  56      4      5       3         1\n",
       "957       4  64      4      5       3         0\n",
       "958       5  66      4      5       3         1\n",
       "959       4  62      3      3       3         0\n",
       "\n",
       "[829 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DearDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "missing_val_count_by_column = (DearDataset.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])   #none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its time to separate our TARGET column from dataset!\n",
    "x_data  = DearDataset.drop('severity',axis=1)\n",
    "y = DearDataset['severity']\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()# why do i need this? Because we want to \"scale\" our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(data=scaler.transform(X_train),columns = X_train.columns,index=X_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = pd.DataFrame(data=scaler.transform(X_test),columns = X_test.columns,index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next you'll need to convert the Pandas dataframes into numpy arrays that can be used by scikit_learn. Create an array that extracts only the feature data we want to work with (age, shape, margin, and density) and another array that contains the classes (severity). You'll also need an array of the feature name labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shapee</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>580 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BI_RADS       age    shapee  margin   density\n",
       "122  0.666667  0.076923  0.333333    0.00  0.333333\n",
       "732  0.666667  0.038462  0.333333    0.00  0.666667\n",
       "14   0.666667  0.538462  0.333333    0.00  0.333333\n",
       "272  0.833333  0.615385  1.000000    0.75  0.666667\n",
       "335  0.833333  0.628205  1.000000    1.00  0.666667\n",
       "464  0.666667  0.448718  0.000000    0.00  0.666667\n",
       "478  0.666667  0.230769  0.000000    0.00  0.666667\n",
       "893  0.666667  0.294872  0.000000    0.00  0.666667\n",
       "138  0.666667  0.448718  0.000000    0.00  0.666667\n",
       "716  1.000000  0.679487  1.000000    0.75  0.666667\n",
       "782  0.666667  0.653846  0.333333    0.00  0.666667\n",
       "99   0.666667  0.192308  0.333333    0.00  0.666667\n",
       "276  0.833333  0.666667  1.000000    1.00  0.666667\n",
       "890  0.666667  0.307692  0.000000    0.00  0.000000\n",
       "489  0.833333  0.371795  1.000000    1.00  0.666667\n",
       "51   0.833333  0.794872  0.666667    1.00  0.666667\n",
       "828  0.666667  0.679487  1.000000    1.00  0.333333\n",
       "440  0.833333  0.487179  0.333333    0.50  0.666667\n",
       "207  0.666667  0.320513  1.000000    0.50  0.666667\n",
       "882  0.500000  0.384615  1.000000    0.75  0.666667\n",
       "784  0.666667  0.602564  0.000000    0.00  0.666667\n",
       "491  0.666667  0.448718  0.000000    0.00  0.666667\n",
       "79   0.833333  0.628205  0.333333    0.75  0.000000\n",
       "652  0.666667  0.217949  0.333333    0.00  0.666667\n",
       "614  0.666667  0.653846  0.000000    0.00  0.666667\n",
       "462  0.666667  0.000000  0.000000    0.00  0.666667\n",
       "741  0.666667  0.538462  0.333333    0.00  0.666667\n",
       "249  0.500000  0.307692  0.000000    0.00  0.666667\n",
       "686  0.666667  0.589744  1.000000    0.75  0.666667\n",
       "342  0.666667  0.179487  0.000000    0.00  0.666667\n",
       "..        ...       ...       ...     ...       ...\n",
       "658  0.833333  0.705128  1.000000    0.75  0.666667\n",
       "771  0.833333  0.641026  1.000000    0.50  0.666667\n",
       "356  0.666667  0.487179  0.666667    0.00  0.666667\n",
       "786  0.666667  0.564103  0.666667    0.50  0.666667\n",
       "653  0.666667  0.641026  0.333333    0.00  0.666667\n",
       "309  0.666667  0.346154  0.000000    0.00  0.666667\n",
       "592  0.833333  0.448718  1.000000    1.00  0.666667\n",
       "763  0.666667  0.256410  0.333333    0.00  0.333333\n",
       "168  0.833333  0.461538  0.666667    1.00  0.666667\n",
       "898  0.666667  0.551282  0.666667    0.75  0.666667\n",
       "502  0.666667  0.166667  0.000000    0.00  0.666667\n",
       "724  0.666667  0.512821  0.333333    0.00  0.333333\n",
       "575  0.666667  0.282051  0.333333    0.00  0.666667\n",
       "62   0.666667  0.500000  0.333333    1.00  0.666667\n",
       "792  0.666667  0.307692  0.666667    0.75  0.333333\n",
       "630  0.666667  0.589744  0.000000    0.00  0.666667\n",
       "477  0.833333  0.423077  1.000000    0.75  0.333333\n",
       "929  0.666667  0.500000  1.000000    0.50  0.666667\n",
       "436  0.833333  0.769231  1.000000    0.75  0.666667\n",
       "368  0.666667  0.628205  1.000000    1.00  0.666667\n",
       "261  0.666667  0.320513  0.000000    0.00  0.666667\n",
       "290  0.833333  0.615385  1.000000    0.75  0.666667\n",
       "830  0.666667  0.423077  0.666667    0.50  0.666667\n",
       "559  0.833333  0.282051  1.000000    0.75  0.666667\n",
       "877  0.833333  0.410256  0.666667    0.25  0.666667\n",
       "160  0.833333  0.628205  1.000000    0.75  0.666667\n",
       "779  0.666667  0.166667  0.000000    0.00  0.666667\n",
       "164  0.666667  0.307692  0.333333    0.00  0.666667\n",
       "17   0.666667  0.525641  0.333333    0.00  0.666667\n",
       "609  0.833333  0.371795  1.000000    1.00  0.666667\n",
       "\n",
       "[580 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our models require the input data to be normalized, so go ahead and normalize the attribute data. Hint: use preprocessing.StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load libraries\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Before moving to K-Fold cross validation and random forests, start by creating a single train/test split of our data. Set aside 75% for training, and 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103  33]\n",
      " [ 35  78]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75       136\n",
      "           1       0.70      0.69      0.70       113\n",
      "\n",
      "    accuracy                           0.73       249\n",
      "   macro avg       0.72      0.72      0.72       249\n",
      "weighted avg       0.73      0.73      0.73       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train,y_train)\n",
    "tree_pred = decision_tree.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,tree_pred))\n",
    "print(classification_report(y_test,tree_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a DecisionTreeClassifier and fit it to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RandomForestClassifier instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107  29]\n",
      " [ 24  89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       136\n",
      "           1       0.75      0.79      0.77       113\n",
      "\n",
      "    accuracy                           0.79       249\n",
      "   macro avg       0.79      0.79      0.79       249\n",
      "weighted avg       0.79      0.79      0.79       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,rf_pred))\n",
    "print(classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Next try using svm.SVC with a linear kernel. How does it compare to the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[111  25]\n",
      " [ 15  98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       136\n",
      "           1       0.80      0.87      0.83       113\n",
      "\n",
      "    accuracy                           0.84       249\n",
      "   macro avg       0.84      0.84      0.84       249\n",
      "weighted avg       0.84      0.84      0.84       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "How about K-Nearest-Neighbors? Hint: use neighbors.KNeighborsClassifier - it's a lot easier than implementing KNN from scratch like we did earlier in the course. Start with a K of 10. K is an example of a hyperparameter - a parameter on the model itself which may need to be tuned for best results on your particular data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[112  24]\n",
      " [ 26  87]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       136\n",
      "           1       0.78      0.77      0.78       113\n",
      "\n",
      "    accuracy                           0.80       249\n",
      "   macro avg       0.80      0.80      0.80       249\n",
      "weighted avg       0.80      0.80      0.80       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KN = KNeighborsClassifier()\n",
    "KN.fit(X_train,y_train)\n",
    "KN_pred = KN.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,KN_pred))\n",
    "print(classification_report(y_test,KN_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Non-Linear SVM\n",
    "\n",
    "svm.SVC may perform differently with different kernels. The choice of kernel is an example of a \"hyperparamter.\" Try the rbf, sigmoid, and poly kernels and see what the best-performing kernel is. Do we have a new winner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='poly')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121  15]\n",
      " [ 25  88]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86       136\n",
      "           1       0.85      0.78      0.81       113\n",
      "\n",
      "    accuracy                           0.84       249\n",
      "   macro avg       0.84      0.83      0.84       249\n",
      "weighted avg       0.84      0.84      0.84       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y2_pred))\n",
    "print(classification_report(y_test,y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We've tried all these fancy techniques, but fundamentally this is just a binary classification problem. Try Logisitic Regression, which is a simple way to tackling this sort of thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[109  27]\n",
      " [ 15  98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84       136\n",
      "           1       0.78      0.87      0.82       113\n",
      "\n",
      "    accuracy                           0.83       249\n",
      "   macro avg       0.83      0.83      0.83       249\n",
      "weighted avg       0.84      0.83      0.83       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regr = LogisticRegression() \n",
    "regr.fit(X_train, y_train) #training the algorithm\n",
    "Log_pred = regr.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,Log_pred))\n",
    "print(classification_report(y_test,Log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: 72.69076305220884 %\n",
      "Random Forest: 78.714859437751 %\n",
      "KNeighbors: 79.91967871485943 %\n",
      "SVM: 83.93574297188755 %\n",
      "Non-Linear SVM 83.93574297188755 %\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree:', accuracy_score(y_test, tree_pred)*100,'%')\n",
    "print('Random Forest:', accuracy_score(y_test, rf_pred)*100,'%')\n",
    "print('KNeighbors:',accuracy_score(y_test, KN_pred)*100,'%')\n",
    "print('SVM:',accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Non-Linear SVM', accuracy_score(y_test, y2_pred)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As a bonus challenge, let's see if an artificial neural network can do even better. You can use Keras to set up a neural network with 1 binary output neuron and see how it performs. Don't be afraid to run a large number of epochs to train the model if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout  # I AM SO SORRY TO USE TF2.0 FML , I NEED TO ADD THOSE OR NOT WORKING\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(7, input_dim=5, activation='relu'))\n",
    "model.add(Dense(7, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.01, patience=8, \\\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 377 samples, validate on 203 samples\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "377/377 [==============================] - 1s 2ms/sample - loss: 0.6715 - acc: 0.4960 - val_loss: 0.6758 - val_acc: 0.5025\n",
      "Epoch 2/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.6672 - acc: 0.4960 - val_loss: 0.6729 - val_acc: 0.5025\n",
      "Epoch 3/200\n",
      "377/377 [==============================] - 0s 56us/sample - loss: 0.6639 - acc: 0.4960 - val_loss: 0.6706 - val_acc: 0.5025\n",
      "Epoch 4/200\n",
      "377/377 [==============================] - 0s 61us/sample - loss: 0.6611 - acc: 0.4960 - val_loss: 0.6683 - val_acc: 0.5025\n",
      "Epoch 5/200\n",
      "377/377 [==============================] - 0s 74us/sample - loss: 0.6585 - acc: 0.4960 - val_loss: 0.6665 - val_acc: 0.5025\n",
      "Epoch 6/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.6561 - acc: 0.4960 - val_loss: 0.6647 - val_acc: 0.5025\n",
      "Epoch 7/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.6537 - acc: 0.4960 - val_loss: 0.6629 - val_acc: 0.5025\n",
      "Epoch 8/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.6515 - acc: 0.4960 - val_loss: 0.6612 - val_acc: 0.5025\n",
      "Epoch 9/200\n",
      "377/377 [==============================] - 0s 63us/sample - loss: 0.6493 - acc: 0.4987 - val_loss: 0.6595 - val_acc: 0.5025\n",
      "Epoch 10/200\n",
      "377/377 [==============================] - 0s 56us/sample - loss: 0.6472 - acc: 0.5013 - val_loss: 0.6574 - val_acc: 0.5025\n",
      "Epoch 11/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.6445 - acc: 0.5013 - val_loss: 0.6555 - val_acc: 0.5025\n",
      "Epoch 12/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.6426 - acc: 0.5066 - val_loss: 0.6540 - val_acc: 0.5025\n",
      "Epoch 13/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.6406 - acc: 0.5093 - val_loss: 0.6527 - val_acc: 0.5074\n",
      "Epoch 14/200\n",
      "377/377 [==============================] - 0s 50us/sample - loss: 0.6391 - acc: 0.5093 - val_loss: 0.6518 - val_acc: 0.5123\n",
      "Epoch 15/200\n",
      "377/377 [==============================] - 0s 63us/sample - loss: 0.6374 - acc: 0.5093 - val_loss: 0.6509 - val_acc: 0.5123\n",
      "Epoch 16/200\n",
      "377/377 [==============================] - 0s 66us/sample - loss: 0.6362 - acc: 0.5093 - val_loss: 0.6500 - val_acc: 0.5123\n",
      "Epoch 17/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.6343 - acc: 0.5093 - val_loss: 0.6486 - val_acc: 0.5172\n",
      "Epoch 18/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.6324 - acc: 0.5172 - val_loss: 0.6470 - val_acc: 0.5961\n",
      "Epoch 19/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.6303 - acc: 0.5809 - val_loss: 0.6450 - val_acc: 0.6158\n",
      "Epoch 20/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.6283 - acc: 0.6207 - val_loss: 0.6432 - val_acc: 0.6305\n",
      "Epoch 21/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.6261 - acc: 0.6499 - val_loss: 0.6415 - val_acc: 0.6207\n",
      "Epoch 22/200\n",
      "377/377 [==============================] - 0s 71us/sample - loss: 0.6243 - acc: 0.6499 - val_loss: 0.6403 - val_acc: 0.6256\n",
      "Epoch 23/200\n",
      "377/377 [==============================] - 0s 69us/sample - loss: 0.6227 - acc: 0.6525 - val_loss: 0.6393 - val_acc: 0.6256\n",
      "Epoch 24/200\n",
      "377/377 [==============================] - 0s 50us/sample - loss: 0.6212 - acc: 0.6525 - val_loss: 0.6383 - val_acc: 0.6305\n",
      "Epoch 25/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.6204 - acc: 0.6525 - val_loss: 0.6377 - val_acc: 0.6305\n",
      "Epoch 26/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.6194 - acc: 0.6525 - val_loss: 0.6368 - val_acc: 0.6355\n",
      "Epoch 27/200\n",
      "377/377 [==============================] - 0s 77us/sample - loss: 0.6184 - acc: 0.6605 - val_loss: 0.6361 - val_acc: 0.6601\n",
      "Epoch 28/200\n",
      "377/377 [==============================] - 0s 56us/sample - loss: 0.6175 - acc: 0.6870 - val_loss: 0.6354 - val_acc: 0.6847\n",
      "Epoch 29/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.6166 - acc: 0.7507 - val_loss: 0.6345 - val_acc: 0.7438\n",
      "Epoch 30/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.6155 - acc: 0.7772 - val_loss: 0.6337 - val_acc: 0.7438\n",
      "Epoch 31/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.6144 - acc: 0.7719 - val_loss: 0.6327 - val_acc: 0.7438\n",
      "Epoch 32/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.6129 - acc: 0.7666 - val_loss: 0.6315 - val_acc: 0.7438\n",
      "Epoch 33/200\n",
      "377/377 [==============================] - 0s 66us/sample - loss: 0.6112 - acc: 0.7666 - val_loss: 0.6304 - val_acc: 0.7438\n",
      "Epoch 34/200\n",
      "377/377 [==============================] - 0s 74us/sample - loss: 0.6094 - acc: 0.7692 - val_loss: 0.6294 - val_acc: 0.7438\n",
      "Epoch 35/200\n",
      "377/377 [==============================] - 0s 56us/sample - loss: 0.6081 - acc: 0.7666 - val_loss: 0.6285 - val_acc: 0.7438\n",
      "Epoch 36/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.6070 - acc: 0.7666 - val_loss: 0.6276 - val_acc: 0.7438\n",
      "Epoch 37/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.6060 - acc: 0.7692 - val_loss: 0.6268 - val_acc: 0.7438\n",
      "Epoch 38/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.6048 - acc: 0.7692 - val_loss: 0.6257 - val_acc: 0.7438\n",
      "Epoch 39/200\n",
      "377/377 [==============================] - 0s 85us/sample - loss: 0.6033 - acc: 0.7692 - val_loss: 0.6247 - val_acc: 0.7438\n",
      "Epoch 40/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.6018 - acc: 0.7692 - val_loss: 0.6236 - val_acc: 0.7438\n",
      "Epoch 41/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.6003 - acc: 0.7692 - val_loss: 0.6225 - val_acc: 0.7438\n",
      "Epoch 42/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5992 - acc: 0.7692 - val_loss: 0.6220 - val_acc: 0.7438\n",
      "Epoch 43/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.5985 - acc: 0.7719 - val_loss: 0.6213 - val_acc: 0.7438\n",
      "Epoch 44/200\n",
      "377/377 [==============================] - 0s 103us/sample - loss: 0.5977 - acc: 0.7745 - val_loss: 0.6205 - val_acc: 0.7389\n",
      "Epoch 45/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.5966 - acc: 0.7745 - val_loss: 0.6197 - val_acc: 0.7389\n",
      "Epoch 46/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5952 - acc: 0.7745 - val_loss: 0.6185 - val_acc: 0.7389\n",
      "Epoch 47/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5935 - acc: 0.7745 - val_loss: 0.6173 - val_acc: 0.7438\n",
      "Epoch 48/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5914 - acc: 0.7745 - val_loss: 0.6162 - val_acc: 0.7438\n",
      "Epoch 49/200\n",
      "377/377 [==============================] - 0s 50us/sample - loss: 0.5900 - acc: 0.7692 - val_loss: 0.6156 - val_acc: 0.7438\n",
      "Epoch 50/200\n",
      "377/377 [==============================] - 0s 81us/sample - loss: 0.5883 - acc: 0.7692 - val_loss: 0.6152 - val_acc: 0.7438\n",
      "Epoch 51/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5872 - acc: 0.7692 - val_loss: 0.6147 - val_acc: 0.7438\n",
      "Epoch 52/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5862 - acc: 0.7692 - val_loss: 0.6139 - val_acc: 0.7438\n",
      "Epoch 53/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5847 - acc: 0.7692 - val_loss: 0.6125 - val_acc: 0.7438\n",
      "Epoch 54/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5831 - acc: 0.7692 - val_loss: 0.6113 - val_acc: 0.7438\n",
      "Epoch 55/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5819 - acc: 0.7692 - val_loss: 0.6103 - val_acc: 0.7438\n",
      "Epoch 56/200\n",
      "377/377 [==============================] - 0s 61us/sample - loss: 0.5808 - acc: 0.7745 - val_loss: 0.6098 - val_acc: 0.7389\n",
      "Epoch 57/200\n",
      "377/377 [==============================] - 0s 63us/sample - loss: 0.5802 - acc: 0.7745 - val_loss: 0.6090 - val_acc: 0.7438\n",
      "Epoch 58/200\n",
      "377/377 [==============================] - 0s 50us/sample - loss: 0.5789 - acc: 0.7745 - val_loss: 0.6084 - val_acc: 0.7438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "377/377 [==============================] - 0s 65us/sample - loss: 0.5777 - acc: 0.7745 - val_loss: 0.6081 - val_acc: 0.7438\n",
      "Epoch 60/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5764 - acc: 0.7692 - val_loss: 0.6078 - val_acc: 0.7438\n",
      "Epoch 61/200\n",
      "377/377 [==============================] - 0s 77us/sample - loss: 0.5754 - acc: 0.7692 - val_loss: 0.6072 - val_acc: 0.7438\n",
      "Epoch 62/200\n",
      "377/377 [==============================] - 0s 53us/sample - loss: 0.5742 - acc: 0.7692 - val_loss: 0.6067 - val_acc: 0.7438\n",
      "Epoch 63/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5732 - acc: 0.7692 - val_loss: 0.6060 - val_acc: 0.7438\n",
      "Epoch 64/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5723 - acc: 0.7692 - val_loss: 0.6060 - val_acc: 0.7438\n",
      "Epoch 65/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5714 - acc: 0.7692 - val_loss: 0.6056 - val_acc: 0.7438\n",
      "Epoch 66/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5706 - acc: 0.7692 - val_loss: 0.6053 - val_acc: 0.7438\n",
      "Epoch 67/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5695 - acc: 0.7692 - val_loss: 0.6050 - val_acc: 0.7438\n",
      "Epoch 68/200\n",
      "377/377 [==============================] - 0s 69us/sample - loss: 0.5686 - acc: 0.7692 - val_loss: 0.6042 - val_acc: 0.7438\n",
      "Epoch 69/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.5674 - acc: 0.7719 - val_loss: 0.6025 - val_acc: 0.7438\n",
      "Epoch 70/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5667 - acc: 0.7772 - val_loss: 0.6019 - val_acc: 0.7438\n",
      "Epoch 71/200\n",
      "377/377 [==============================] - 0s 24us/sample - loss: 0.5668 - acc: 0.7825 - val_loss: 0.6016 - val_acc: 0.7438\n",
      "Epoch 72/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5666 - acc: 0.7825 - val_loss: 0.6012 - val_acc: 0.7537\n",
      "Epoch 73/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5664 - acc: 0.7798 - val_loss: 0.6009 - val_acc: 0.7438\n",
      "Epoch 74/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5656 - acc: 0.7798 - val_loss: 0.6002 - val_acc: 0.7488\n",
      "Epoch 75/200\n",
      "377/377 [==============================] - 0s 71us/sample - loss: 0.5648 - acc: 0.7798 - val_loss: 0.5997 - val_acc: 0.7488\n",
      "Epoch 76/200\n",
      "377/377 [==============================] - 0s 74us/sample - loss: 0.5640 - acc: 0.7798 - val_loss: 0.5991 - val_acc: 0.7537\n",
      "Epoch 77/200\n",
      "377/377 [==============================] - 0s 50us/sample - loss: 0.5628 - acc: 0.7825 - val_loss: 0.5985 - val_acc: 0.7488\n",
      "Epoch 78/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5618 - acc: 0.7851 - val_loss: 0.5980 - val_acc: 0.7488\n",
      "Epoch 79/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5611 - acc: 0.7851 - val_loss: 0.5976 - val_acc: 0.7438\n",
      "Epoch 80/200\n",
      "377/377 [==============================] - 0s 58us/sample - loss: 0.5601 - acc: 0.7825 - val_loss: 0.5972 - val_acc: 0.7438\n",
      "Epoch 81/200\n",
      "377/377 [==============================] - 0s 53us/sample - loss: 0.5593 - acc: 0.7825 - val_loss: 0.5968 - val_acc: 0.7438\n",
      "Epoch 82/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5585 - acc: 0.7798 - val_loss: 0.5966 - val_acc: 0.7389\n",
      "Epoch 83/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5580 - acc: 0.7798 - val_loss: 0.5963 - val_acc: 0.7389\n",
      "Epoch 84/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5572 - acc: 0.7798 - val_loss: 0.5956 - val_acc: 0.7438\n",
      "Epoch 85/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5566 - acc: 0.7851 - val_loss: 0.5949 - val_acc: 0.7488\n",
      "Epoch 86/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5560 - acc: 0.7798 - val_loss: 0.5944 - val_acc: 0.7488\n",
      "Epoch 87/200\n",
      "377/377 [==============================] - 0s 61us/sample - loss: 0.5559 - acc: 0.7825 - val_loss: 0.5939 - val_acc: 0.7438\n",
      "Epoch 88/200\n",
      "377/377 [==============================] - 0s 53us/sample - loss: 0.5555 - acc: 0.7878 - val_loss: 0.5934 - val_acc: 0.7438\n",
      "Epoch 89/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5548 - acc: 0.7878 - val_loss: 0.5928 - val_acc: 0.7438\n",
      "Epoch 90/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5536 - acc: 0.7878 - val_loss: 0.5923 - val_acc: 0.7438\n",
      "Epoch 91/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5524 - acc: 0.7798 - val_loss: 0.5919 - val_acc: 0.7488\n",
      "Epoch 92/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5514 - acc: 0.7798 - val_loss: 0.5916 - val_acc: 0.7488\n",
      "Epoch 93/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5506 - acc: 0.7798 - val_loss: 0.5912 - val_acc: 0.7488\n",
      "Epoch 94/200\n",
      "377/377 [==============================] - 0s 87us/sample - loss: 0.5498 - acc: 0.7798 - val_loss: 0.5910 - val_acc: 0.7488\n",
      "Epoch 95/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.5491 - acc: 0.7798 - val_loss: 0.5904 - val_acc: 0.7488\n",
      "Epoch 96/200\n",
      "377/377 [==============================] - 0s 63us/sample - loss: 0.5487 - acc: 0.7798 - val_loss: 0.5897 - val_acc: 0.7438\n",
      "Epoch 97/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5483 - acc: 0.7825 - val_loss: 0.5894 - val_acc: 0.7438\n",
      "Epoch 98/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5477 - acc: 0.7825 - val_loss: 0.5889 - val_acc: 0.7438\n",
      "Epoch 99/200\n",
      "377/377 [==============================] - 0s 77us/sample - loss: 0.5472 - acc: 0.7851 - val_loss: 0.5884 - val_acc: 0.7438\n",
      "Epoch 100/200\n",
      "377/377 [==============================] - 0s 56us/sample - loss: 0.5468 - acc: 0.7825 - val_loss: 0.5881 - val_acc: 0.7438\n",
      "Epoch 101/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5460 - acc: 0.7825 - val_loss: 0.5879 - val_acc: 0.7488\n",
      "Epoch 102/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5454 - acc: 0.7825 - val_loss: 0.5877 - val_acc: 0.7488\n",
      "Epoch 103/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5452 - acc: 0.7825 - val_loss: 0.5880 - val_acc: 0.7488\n",
      "Epoch 104/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5445 - acc: 0.7825 - val_loss: 0.5878 - val_acc: 0.7488\n",
      "Epoch 105/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.5438 - acc: 0.7798 - val_loss: 0.5868 - val_acc: 0.7488\n",
      "Epoch 106/200\n",
      "377/377 [==============================] - 0s 77us/sample - loss: 0.5433 - acc: 0.7825 - val_loss: 0.5861 - val_acc: 0.7488\n",
      "Epoch 107/200\n",
      "377/377 [==============================] - 0s 50us/sample - loss: 0.5428 - acc: 0.7825 - val_loss: 0.5858 - val_acc: 0.7488\n",
      "Epoch 108/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5420 - acc: 0.7825 - val_loss: 0.5858 - val_acc: 0.7488\n",
      "Epoch 109/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5416 - acc: 0.7798 - val_loss: 0.5861 - val_acc: 0.7488\n",
      "Epoch 110/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5411 - acc: 0.7798 - val_loss: 0.5863 - val_acc: 0.7438\n",
      "Epoch 111/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5407 - acc: 0.7798 - val_loss: 0.5864 - val_acc: 0.7438\n",
      "Epoch 112/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5405 - acc: 0.7825 - val_loss: 0.5865 - val_acc: 0.7438\n",
      "Epoch 113/200\n",
      "377/377 [==============================] - 0s 63us/sample - loss: 0.5395 - acc: 0.7825 - val_loss: 0.5854 - val_acc: 0.7488\n",
      "Epoch 114/200\n",
      "377/377 [==============================] - 0s 56us/sample - loss: 0.5386 - acc: 0.7825 - val_loss: 0.5847 - val_acc: 0.7488\n",
      "Epoch 115/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5379 - acc: 0.7825 - val_loss: 0.5845 - val_acc: 0.7488\n",
      "Epoch 116/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5375 - acc: 0.7825 - val_loss: 0.5838 - val_acc: 0.7438\n",
      "Epoch 117/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5369 - acc: 0.7825 - val_loss: 0.5839 - val_acc: 0.7488\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5367 - acc: 0.7825 - val_loss: 0.5842 - val_acc: 0.7488\n",
      "Epoch 119/200\n",
      "377/377 [==============================] - 0s 66us/sample - loss: 0.5365 - acc: 0.7798 - val_loss: 0.5843 - val_acc: 0.7488\n",
      "Epoch 120/200\n",
      "377/377 [==============================] - 0s 53us/sample - loss: 0.5360 - acc: 0.7798 - val_loss: 0.5839 - val_acc: 0.7488\n",
      "Epoch 121/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5353 - acc: 0.7825 - val_loss: 0.5830 - val_acc: 0.7438\n",
      "Epoch 122/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5347 - acc: 0.7825 - val_loss: 0.5826 - val_acc: 0.7438\n",
      "Epoch 123/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5341 - acc: 0.7825 - val_loss: 0.5827 - val_acc: 0.7488\n",
      "Epoch 124/200\n",
      "377/377 [==============================] - 0s 61us/sample - loss: 0.5339 - acc: 0.7825 - val_loss: 0.5826 - val_acc: 0.7488\n",
      "Epoch 125/200\n",
      "377/377 [==============================] - 0s 82us/sample - loss: 0.5331 - acc: 0.7825 - val_loss: 0.5814 - val_acc: 0.7438\n",
      "Epoch 126/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5322 - acc: 0.7851 - val_loss: 0.5804 - val_acc: 0.7438\n",
      "Epoch 127/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5316 - acc: 0.7878 - val_loss: 0.5800 - val_acc: 0.7438\n",
      "Epoch 128/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5311 - acc: 0.7878 - val_loss: 0.5798 - val_acc: 0.7438\n",
      "Epoch 129/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5306 - acc: 0.7878 - val_loss: 0.5795 - val_acc: 0.7438\n",
      "Epoch 130/200\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.5681 - acc: 0.733 - 0s 26us/sample - loss: 0.5300 - acc: 0.7878 - val_loss: 0.5792 - val_acc: 0.7438\n",
      "Epoch 131/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.5295 - acc: 0.7905 - val_loss: 0.5787 - val_acc: 0.7438\n",
      "Epoch 132/200\n",
      "377/377 [==============================] - 0s 85us/sample - loss: 0.5286 - acc: 0.7931 - val_loss: 0.5777 - val_acc: 0.7438\n",
      "Epoch 133/200\n",
      "377/377 [==============================] - 0s 71us/sample - loss: 0.5281 - acc: 0.7958 - val_loss: 0.5772 - val_acc: 0.7438\n",
      "Epoch 134/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.5275 - acc: 0.7984 - val_loss: 0.5768 - val_acc: 0.7438\n",
      "Epoch 135/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5270 - acc: 0.7958 - val_loss: 0.5768 - val_acc: 0.7438\n",
      "Epoch 136/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5267 - acc: 0.7958 - val_loss: 0.5767 - val_acc: 0.7438\n",
      "Epoch 137/200\n",
      "377/377 [==============================] - 0s 82us/sample - loss: 0.5262 - acc: 0.7958 - val_loss: 0.5762 - val_acc: 0.7438\n",
      "Epoch 138/200\n",
      "377/377 [==============================] - 0s 106us/sample - loss: 0.5256 - acc: 0.7958 - val_loss: 0.5759 - val_acc: 0.7438\n",
      "Epoch 139/200\n",
      "377/377 [==============================] - 0s 64us/sample - loss: 0.5250 - acc: 0.8011 - val_loss: 0.5757 - val_acc: 0.7438\n",
      "Epoch 140/200\n",
      "377/377 [==============================] - 0s 40us/sample - loss: 0.5246 - acc: 0.7984 - val_loss: 0.5756 - val_acc: 0.7438\n",
      "Epoch 141/200\n",
      "377/377 [==============================] - 0s 56us/sample - loss: 0.5241 - acc: 0.7984 - val_loss: 0.5751 - val_acc: 0.7438\n",
      "Epoch 142/200\n",
      "377/377 [==============================] - 0s 69us/sample - loss: 0.5234 - acc: 0.8011 - val_loss: 0.5750 - val_acc: 0.7438\n",
      "Epoch 143/200\n",
      "377/377 [==============================] - 0s 54us/sample - loss: 0.5231 - acc: 0.8011 - val_loss: 0.5751 - val_acc: 0.7438\n",
      "Epoch 144/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5227 - acc: 0.8011 - val_loss: 0.5743 - val_acc: 0.7438\n",
      "Epoch 145/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5219 - acc: 0.8037 - val_loss: 0.5742 - val_acc: 0.7438\n",
      "Epoch 146/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5215 - acc: 0.8011 - val_loss: 0.5745 - val_acc: 0.7438\n",
      "Epoch 147/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5212 - acc: 0.8037 - val_loss: 0.5746 - val_acc: 0.7438\n",
      "Epoch 148/200\n",
      "377/377 [==============================] - 0s 70us/sample - loss: 0.5210 - acc: 0.8037 - val_loss: 0.5741 - val_acc: 0.7438\n",
      "Epoch 149/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.5204 - acc: 0.8011 - val_loss: 0.5733 - val_acc: 0.7438\n",
      "Epoch 150/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5199 - acc: 0.8037 - val_loss: 0.5729 - val_acc: 0.7438\n",
      "Epoch 151/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5194 - acc: 0.8037 - val_loss: 0.5726 - val_acc: 0.7438\n",
      "Epoch 152/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5190 - acc: 0.8037 - val_loss: 0.5728 - val_acc: 0.7438\n",
      "Epoch 153/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5186 - acc: 0.8011 - val_loss: 0.5730 - val_acc: 0.7438\n",
      "Epoch 154/200\n",
      "377/377 [==============================] - 0s 63us/sample - loss: 0.5185 - acc: 0.7984 - val_loss: 0.5735 - val_acc: 0.7438\n",
      "Epoch 155/200\n",
      "377/377 [==============================] - 0s 82us/sample - loss: 0.5182 - acc: 0.7984 - val_loss: 0.5732 - val_acc: 0.7438\n",
      "Epoch 156/200\n",
      "377/377 [==============================] - 0s 58us/sample - loss: 0.5174 - acc: 0.8011 - val_loss: 0.5713 - val_acc: 0.7438\n",
      "Epoch 157/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.5163 - acc: 0.8011 - val_loss: 0.5701 - val_acc: 0.7488\n",
      "Epoch 158/200\n",
      "377/377 [==============================] - 0s 26us/sample - loss: 0.5157 - acc: 0.8037 - val_loss: 0.5688 - val_acc: 0.7438\n",
      "Epoch 159/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5154 - acc: 0.7984 - val_loss: 0.5683 - val_acc: 0.7488\n",
      "Epoch 160/200\n",
      "377/377 [==============================] - 0s 69us/sample - loss: 0.5148 - acc: 0.7984 - val_loss: 0.5682 - val_acc: 0.7438\n",
      "Epoch 161/200\n",
      "377/377 [==============================] - 0s 26us/sample - loss: 0.5146 - acc: 0.8037 - val_loss: 0.5686 - val_acc: 0.7438\n",
      "Epoch 162/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5139 - acc: 0.8037 - val_loss: 0.5686 - val_acc: 0.7438\n",
      "Epoch 163/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5135 - acc: 0.8037 - val_loss: 0.5686 - val_acc: 0.7488\n",
      "Epoch 164/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5130 - acc: 0.8037 - val_loss: 0.5682 - val_acc: 0.7438\n",
      "Epoch 165/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5126 - acc: 0.8037 - val_loss: 0.5679 - val_acc: 0.7438\n",
      "Epoch 166/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5121 - acc: 0.8011 - val_loss: 0.5681 - val_acc: 0.7438\n",
      "Epoch 167/200\n",
      "377/377 [==============================] - 0s 69us/sample - loss: 0.5118 - acc: 0.8011 - val_loss: 0.5680 - val_acc: 0.7438\n",
      "Epoch 168/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.5113 - acc: 0.8011 - val_loss: 0.5678 - val_acc: 0.7438\n",
      "Epoch 169/200\n",
      "377/377 [==============================] - 0s 50us/sample - loss: 0.5109 - acc: 0.8011 - val_loss: 0.5680 - val_acc: 0.7488\n",
      "Epoch 170/200\n",
      "377/377 [==============================] - 0s 42us/sample - loss: 0.5108 - acc: 0.8011 - val_loss: 0.5680 - val_acc: 0.7488\n",
      "Epoch 171/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5102 - acc: 0.8011 - val_loss: 0.5675 - val_acc: 0.7488\n",
      "Epoch 172/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5096 - acc: 0.8011 - val_loss: 0.5667 - val_acc: 0.7438\n",
      "Epoch 173/200\n",
      "377/377 [==============================] - 0s 58us/sample - loss: 0.5087 - acc: 0.8037 - val_loss: 0.5657 - val_acc: 0.7389\n",
      "Epoch 174/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.5080 - acc: 0.7984 - val_loss: 0.5651 - val_acc: 0.7389\n",
      "Epoch 175/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5074 - acc: 0.7984 - val_loss: 0.5649 - val_acc: 0.7438\n",
      "Epoch 176/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5069 - acc: 0.7984 - val_loss: 0.5647 - val_acc: 0.7438\n",
      "Epoch 177/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5065 - acc: 0.7984 - val_loss: 0.5649 - val_acc: 0.7389\n",
      "Epoch 178/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.5064 - acc: 0.8011 - val_loss: 0.5653 - val_acc: 0.7389\n",
      "Epoch 179/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5061 - acc: 0.8011 - val_loss: 0.5655 - val_acc: 0.7389\n",
      "Epoch 180/200\n",
      "377/377 [==============================] - 0s 71us/sample - loss: 0.5057 - acc: 0.8011 - val_loss: 0.5658 - val_acc: 0.7438\n",
      "Epoch 181/200\n",
      "377/377 [==============================] - 0s 69us/sample - loss: 0.5060 - acc: 0.8011 - val_loss: 0.5662 - val_acc: 0.7438\n",
      "Epoch 182/200\n",
      "377/377 [==============================] - 0s 58us/sample - loss: 0.5054 - acc: 0.8011 - val_loss: 0.5650 - val_acc: 0.7389\n",
      "Epoch 183/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.5045 - acc: 0.8011 - val_loss: 0.5644 - val_acc: 0.7389\n",
      "Epoch 184/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.5040 - acc: 0.8011 - val_loss: 0.5643 - val_acc: 0.7389\n",
      "Epoch 185/200\n",
      "377/377 [==============================] - 0s 79us/sample - loss: 0.5038 - acc: 0.8037 - val_loss: 0.5644 - val_acc: 0.7488\n",
      "Epoch 186/200\n",
      "377/377 [==============================] - 0s 53us/sample - loss: 0.5037 - acc: 0.8037 - val_loss: 0.5642 - val_acc: 0.7488\n",
      "Epoch 187/200\n",
      "377/377 [==============================] - 0s 53us/sample - loss: 0.5032 - acc: 0.8037 - val_loss: 0.5635 - val_acc: 0.7488\n",
      "Epoch 188/200\n",
      "377/377 [==============================] - 0s 34us/sample - loss: 0.5025 - acc: 0.8037 - val_loss: 0.5629 - val_acc: 0.7488\n",
      "Epoch 189/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5019 - acc: 0.8037 - val_loss: 0.5617 - val_acc: 0.7488\n",
      "Epoch 190/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5004 - acc: 0.7984 - val_loss: 0.5592 - val_acc: 0.7438\n",
      "Epoch 191/200\n",
      "377/377 [==============================] - 0s 29us/sample - loss: 0.5000 - acc: 0.7984 - val_loss: 0.5583 - val_acc: 0.7438\n",
      "Epoch 192/200\n",
      "377/377 [==============================] - 0s 66us/sample - loss: 0.5001 - acc: 0.8037 - val_loss: 0.5583 - val_acc: 0.7537\n",
      "Epoch 193/200\n",
      "377/377 [==============================] - 0s 56us/sample - loss: 0.5021 - acc: 0.8117 - val_loss: 0.5588 - val_acc: 0.7488\n",
      "Epoch 194/200\n",
      "377/377 [==============================] - 0s 45us/sample - loss: 0.5033 - acc: 0.8170 - val_loss: 0.5589 - val_acc: 0.7438\n",
      "Epoch 195/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.5025 - acc: 0.8170 - val_loss: 0.5579 - val_acc: 0.7488\n",
      "Epoch 196/200\n",
      "377/377 [==============================] - 0s 32us/sample - loss: 0.5005 - acc: 0.8143 - val_loss: 0.5568 - val_acc: 0.7537\n",
      "Epoch 197/200\n",
      "377/377 [==============================] - 0s 48us/sample - loss: 0.4987 - acc: 0.8090 - val_loss: 0.5563 - val_acc: 0.7488\n",
      "Epoch 198/200\n",
      "377/377 [==============================] - 0s 85us/sample - loss: 0.4982 - acc: 0.8064 - val_loss: 0.5560 - val_acc: 0.7537\n",
      "Epoch 199/200\n",
      "377/377 [==============================] - 0s 64us/sample - loss: 0.4979 - acc: 0.8090 - val_loss: 0.5555 - val_acc: 0.7488\n",
      "Epoch 200/200\n",
      "377/377 [==============================] - 0s 37us/sample - loss: 0.4967 - acc: 0.8037 - val_loss: 0.5552 - val_acc: 0.7488\n"
     ]
    }
   ],
   "source": [
    "model_without_early=model.fit(X_train, y_train, validation_split=0.35, epochs=200, batch_size=75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history): #Plot Accuracy or Loss as a Function of Number of Epoch\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_history(model_without_early)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#try changing: epoch, batch size and these values:: \n",
    "model.add(Dense(7, input_dim=5, activation='relu'))\n",
    "model.add(Dense(7, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# you can increase the number of nodes or you can even add another layer!\n",
    "#try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
